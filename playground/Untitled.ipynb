{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dadrah'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7a8b45c9175d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#import setGPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdadrah\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplayground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_gradient_tape\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtegrta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdadrah\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_strategy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdadrah\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dadrah'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "#import setGPU\n",
    "from importlib import reload\n",
    "import dadrah.playground.test_gradient_tape as tegrta\n",
    "import dadrah.selection.loss_strategy as ls\n",
    "import dadrah.selection.discriminator as disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.uniform([3000, 1], minval=1, maxval=100, dtype=tf.float32)\n",
    "y = tf.random.uniform([3000,1], minval=1, maxval=10, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3000, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile = 0.1\n",
    "strategy = ls.combine_loss_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Start of epoch 0\n",
      "Training loss (for one batch) at step 0: 142.17120361328125\n",
      "Training loss (for one batch) at step 2: 119.99320983886719\n",
      "Training loss (for one batch) at step 4: 84.17875671386719\n",
      "Training loss (for one batch) at step 6: 54.75967025756836\n",
      "Training loss (for one batch) at step 8: 59.2042236328125\n",
      "Training loss (for one batch) at step 10: 69.57876586914062\n",
      "Training loss (for one batch) at step 12: 62.67546081542969\n",
      "Training loss (for one batch) at step 14: 54.283287048339844\n",
      "Training loss (for one batch) at step 16: 51.496402740478516\n",
      "Training loss (for one batch) at step 18: 58.47557067871094\n",
      "Training loss (for one batch) at step 20: 59.661895751953125\n",
      "Training loss (for one batch) at step 22: 58.61177062988281\n",
      "\n",
      "Start of epoch 1\n",
      "Training loss (for one batch) at step 0: 52.76274490356445\n",
      "Training loss (for one batch) at step 2: 53.90526580810547\n",
      "Training loss (for one batch) at step 4: 49.281837463378906\n",
      "Training loss (for one batch) at step 6: 53.007118225097656\n",
      "Training loss (for one batch) at step 8: 55.670745849609375\n",
      "Training loss (for one batch) at step 10: 56.913143157958984\n",
      "Training loss (for one batch) at step 12: 52.88890838623047\n",
      "Training loss (for one batch) at step 14: 51.19029998779297\n",
      "Training loss (for one batch) at step 16: 51.64230728149414\n",
      "Training loss (for one batch) at step 18: 57.3472900390625\n",
      "Training loss (for one batch) at step 20: 56.77628707885742\n",
      "Training loss (for one batch) at step 22: 56.29369354248047\n",
      "\n",
      "Start of epoch 2\n",
      "Training loss (for one batch) at step 0: 51.400699615478516\n",
      "Training loss (for one batch) at step 2: 52.491310119628906\n",
      "Training loss (for one batch) at step 4: 49.040306091308594\n",
      "Training loss (for one batch) at step 6: 52.43000030517578\n",
      "Training loss (for one batch) at step 8: 55.042327880859375\n",
      "Training loss (for one batch) at step 10: 55.873756408691406\n",
      "Training loss (for one batch) at step 12: 51.95214080810547\n",
      "Training loss (for one batch) at step 14: 50.696468353271484\n",
      "Training loss (for one batch) at step 16: 51.855377197265625\n",
      "Training loss (for one batch) at step 18: 57.14997100830078\n",
      "Training loss (for one batch) at step 20: 56.34794616699219\n",
      "Training loss (for one batch) at step 22: 55.72968292236328\n",
      "\n",
      "Start of epoch 3\n",
      "Training loss (for one batch) at step 0: 51.42573165893555\n",
      "Training loss (for one batch) at step 2: 52.197147369384766\n",
      "Training loss (for one batch) at step 4: 49.058631896972656\n",
      "Training loss (for one batch) at step 6: 52.271915435791016\n",
      "Training loss (for one batch) at step 8: 54.955909729003906\n",
      "Training loss (for one batch) at step 10: 55.79328155517578\n",
      "Training loss (for one batch) at step 12: 51.772789001464844\n",
      "Training loss (for one batch) at step 14: 50.58441162109375\n",
      "Training loss (for one batch) at step 16: 51.927650451660156\n",
      "Training loss (for one batch) at step 18: 57.017486572265625\n",
      "Training loss (for one batch) at step 20: 56.18434524536133\n",
      "Training loss (for one batch) at step 22: 55.247703552246094\n",
      "\n",
      "Start of epoch 4\n",
      "Training loss (for one batch) at step 0: 51.96382141113281\n",
      "Training loss (for one batch) at step 2: 52.0928955078125\n",
      "Training loss (for one batch) at step 4: 49.080142974853516\n",
      "Training loss (for one batch) at step 6: 51.982051849365234\n",
      "Training loss (for one batch) at step 8: 55.128517150878906\n",
      "Training loss (for one batch) at step 10: 55.85942459106445\n",
      "Training loss (for one batch) at step 12: 51.605445861816406\n",
      "Training loss (for one batch) at step 14: 50.45691680908203\n",
      "Training loss (for one batch) at step 16: 51.82404327392578\n",
      "Training loss (for one batch) at step 18: 56.85652160644531\n",
      "Training loss (for one batch) at step 20: 55.84912109375\n",
      "Training loss (for one batch) at step 22: 54.98736572265625\n",
      "\n",
      "Start of epoch 5\n",
      "Training loss (for one batch) at step 0: 52.14524841308594\n",
      "Training loss (for one batch) at step 2: 52.10369873046875\n",
      "Training loss (for one batch) at step 4: 49.07020568847656\n",
      "Training loss (for one batch) at step 6: 51.906394958496094\n",
      "Training loss (for one batch) at step 8: 55.20832824707031\n",
      "Training loss (for one batch) at step 10: 55.900352478027344\n",
      "Training loss (for one batch) at step 12: 51.594078063964844\n",
      "Training loss (for one batch) at step 14: 50.500946044921875\n",
      "Training loss (for one batch) at step 16: 51.72510528564453\n",
      "Training loss (for one batch) at step 18: 56.77606201171875\n",
      "Training loss (for one batch) at step 20: 55.77619552612305\n",
      "Training loss (for one batch) at step 22: 54.85699462890625\n",
      "\n",
      "Start of epoch 6\n",
      "Training loss (for one batch) at step 0: 52.38022232055664\n",
      "Training loss (for one batch) at step 2: 52.12297058105469\n",
      "Training loss (for one batch) at step 4: 49.095149993896484\n",
      "Training loss (for one batch) at step 6: 51.75818634033203\n",
      "Training loss (for one batch) at step 8: 55.37282180786133\n",
      "Training loss (for one batch) at step 10: 56.052406311035156\n",
      "Training loss (for one batch) at step 12: 51.51365661621094\n",
      "Training loss (for one batch) at step 14: 50.44202423095703\n",
      "Training loss (for one batch) at step 16: 51.55011749267578\n",
      "Training loss (for one batch) at step 18: 56.46763229370117\n",
      "Training loss (for one batch) at step 20: 55.48870086669922\n",
      "Training loss (for one batch) at step 22: 54.645301818847656\n",
      "\n",
      "Start of epoch 7\n",
      "Training loss (for one batch) at step 0: 52.35902404785156\n",
      "Training loss (for one batch) at step 2: 52.0651741027832\n",
      "Training loss (for one batch) at step 4: 49.19624710083008\n",
      "Training loss (for one batch) at step 6: 51.64155960083008\n",
      "Training loss (for one batch) at step 8: 55.363311767578125\n",
      "Training loss (for one batch) at step 10: 55.8524169921875\n",
      "Training loss (for one batch) at step 12: 51.564476013183594\n",
      "Training loss (for one batch) at step 14: 50.667362213134766\n",
      "Training loss (for one batch) at step 16: 51.35125732421875\n",
      "Training loss (for one batch) at step 18: 56.47542953491211\n",
      "Training loss (for one batch) at step 20: 55.64720153808594\n",
      "Training loss (for one batch) at step 22: 54.76554870605469\n",
      "\n",
      "Start of epoch 8\n",
      "Training loss (for one batch) at step 0: 52.27156066894531\n",
      "Training loss (for one batch) at step 2: 52.0621337890625\n",
      "Training loss (for one batch) at step 4: 49.204010009765625\n",
      "Training loss (for one batch) at step 6: 51.64634704589844\n",
      "Training loss (for one batch) at step 8: 55.37677001953125\n",
      "Training loss (for one batch) at step 10: 55.866798400878906\n",
      "Training loss (for one batch) at step 12: 51.5433464050293\n",
      "Training loss (for one batch) at step 14: 50.62086868286133\n",
      "Training loss (for one batch) at step 16: 51.3984375\n",
      "Training loss (for one batch) at step 18: 56.49018859863281\n",
      "Training loss (for one batch) at step 20: 55.61671447753906\n",
      "Training loss (for one batch) at step 22: 54.70806884765625\n",
      "\n",
      "Start of epoch 9\n",
      "Training loss (for one batch) at step 0: 52.42833709716797\n",
      "Training loss (for one batch) at step 2: 52.0695686340332\n",
      "Training loss (for one batch) at step 4: 49.19348907470703\n",
      "Training loss (for one batch) at step 6: 51.62548828125\n",
      "Training loss (for one batch) at step 8: 55.41701889038086\n",
      "Training loss (for one batch) at step 10: 55.90241241455078\n",
      "Training loss (for one batch) at step 12: 51.538326263427734\n",
      "Training loss (for one batch) at step 14: 50.613441467285156\n",
      "Training loss (for one batch) at step 16: 51.38934326171875\n",
      "Training loss (for one batch) at step 18: 56.47826385498047\n",
      "Training loss (for one batch) at step 20: 55.645751953125\n",
      "Training loss (for one batch) at step 22: 54.79745101928711\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "feature_normalization (Featu (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 30)                60        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 30)                930       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 31        \n",
      "_________________________________________________________________\n",
      "feature_un_normalization (Fe (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 2,881\n",
      "Trainable params: 2,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import dadrah.selection.quantile_regression as qure\n",
    "reload(qure)\n",
    "reload(disc)\n",
    "discriminator = disc.QRDiscriminator(quantile=quantile, loss_strategy=strategy, epochs=10, n_nodes=30)\n",
    "discriminator.fit(x, y)\n",
    "print(discriminator.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = tf.random.uniform([300, 1], minval=1, maxval=100, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(300, 1), dtype=float32, numpy=\n",
       "array([[ 9.920164 ],\n",
       "       [61.444466 ],\n",
       "       [79.72258  ],\n",
       "       [99.88019  ],\n",
       "       [11.634789 ],\n",
       "       [64.567345 ],\n",
       "       [41.09856  ],\n",
       "       [35.97979  ],\n",
       "       [65.91308  ],\n",
       "       [77.14003  ],\n",
       "       [38.83267  ],\n",
       "       [34.518124 ],\n",
       "       [40.181625 ],\n",
       "       [93.052895 ],\n",
       "       [99.196495 ],\n",
       "       [11.585918 ],\n",
       "       [92.9842   ],\n",
       "       [23.480211 ],\n",
       "       [35.9555   ],\n",
       "       [94.17641  ],\n",
       "       [24.325344 ],\n",
       "       [92.15333  ],\n",
       "       [45.68191  ],\n",
       "       [64.08234  ],\n",
       "       [56.854847 ],\n",
       "       [45.84055  ],\n",
       "       [ 8.648931 ],\n",
       "       [44.039642 ],\n",
       "       [58.54842  ],\n",
       "       [64.27069  ],\n",
       "       [51.68272  ],\n",
       "       [10.009503 ],\n",
       "       [36.914345 ],\n",
       "       [46.982475 ],\n",
       "       [65.73589  ],\n",
       "       [55.834908 ],\n",
       "       [55.98276  ],\n",
       "       [14.50957  ],\n",
       "       [96.24013  ],\n",
       "       [64.313866 ],\n",
       "       [74.743774 ],\n",
       "       [26.961506 ],\n",
       "       [27.457981 ],\n",
       "       [47.651432 ],\n",
       "       [34.95206  ],\n",
       "       [52.265972 ],\n",
       "       [14.910286 ],\n",
       "       [30.657402 ],\n",
       "       [65.82034  ],\n",
       "       [88.23235  ],\n",
       "       [45.77929  ],\n",
       "       [97.995865 ],\n",
       "       [62.954742 ],\n",
       "       [23.576384 ],\n",
       "       [29.87227  ],\n",
       "       [54.21656  ],\n",
       "       [64.8093   ],\n",
       "       [42.53352  ],\n",
       "       [96.2039   ],\n",
       "       [ 7.931669 ],\n",
       "       [61.06824  ],\n",
       "       [25.221035 ],\n",
       "       [ 9.980283 ],\n",
       "       [95.81815  ],\n",
       "       [33.242462 ],\n",
       "       [64.77581  ],\n",
       "       [19.906118 ],\n",
       "       [37.956036 ],\n",
       "       [55.990173 ],\n",
       "       [95.25363  ],\n",
       "       [51.590385 ],\n",
       "       [50.766613 ],\n",
       "       [ 2.343791 ],\n",
       "       [40.994755 ],\n",
       "       [81.74235  ],\n",
       "       [47.073288 ],\n",
       "       [26.401337 ],\n",
       "       [ 1.6518444],\n",
       "       [74.52959  ],\n",
       "       [27.232037 ],\n",
       "       [57.93472  ],\n",
       "       [23.203495 ],\n",
       "       [ 6.474877 ],\n",
       "       [93.07081  ],\n",
       "       [84.65106  ],\n",
       "       [84.95379  ],\n",
       "       [51.543106 ],\n",
       "       [27.293287 ],\n",
       "       [85.212    ],\n",
       "       [99.51857  ],\n",
       "       [36.146053 ],\n",
       "       [83.071365 ],\n",
       "       [34.51004  ],\n",
       "       [ 5.9539604],\n",
       "       [52.977215 ],\n",
       "       [70.37012  ],\n",
       "       [18.27537  ],\n",
       "       [48.27475  ],\n",
       "       [86.16727  ],\n",
       "       [74.47536  ],\n",
       "       [61.2508   ],\n",
       "       [22.854    ],\n",
       "       [79.00585  ],\n",
       "       [86.57284  ],\n",
       "       [42.553337 ],\n",
       "       [55.844315 ],\n",
       "       [55.659687 ],\n",
       "       [37.157825 ],\n",
       "       [51.415886 ],\n",
       "       [34.45575  ],\n",
       "       [80.57582  ],\n",
       "       [81.91048  ],\n",
       "       [65.740166 ],\n",
       "       [75.2      ],\n",
       "       [88.99869  ],\n",
       "       [17.302553 ],\n",
       "       [81.50557  ],\n",
       "       [89.89868  ],\n",
       "       [85.76735  ],\n",
       "       [87.771164 ],\n",
       "       [42.18793  ],\n",
       "       [93.78008  ],\n",
       "       [48.949844 ],\n",
       "       [23.436922 ],\n",
       "       [17.189293 ],\n",
       "       [31.97077  ],\n",
       "       [71.910965 ],\n",
       "       [94.61804  ],\n",
       "       [39.286007 ],\n",
       "       [33.58286  ],\n",
       "       [71.32243  ],\n",
       "       [45.050392 ],\n",
       "       [96.40006  ],\n",
       "       [28.65235  ],\n",
       "       [77.516075 ],\n",
       "       [68.17978  ],\n",
       "       [61.423637 ],\n",
       "       [64.95055  ],\n",
       "       [42.87466  ],\n",
       "       [ 6.5517883],\n",
       "       [69.35466  ],\n",
       "       [50.910156 ],\n",
       "       [41.144318 ],\n",
       "       [92.325096 ],\n",
       "       [ 2.4382756],\n",
       "       [96.245    ],\n",
       "       [82.29426  ],\n",
       "       [41.461193 ],\n",
       "       [31.903759 ],\n",
       "       [23.199802 ],\n",
       "       [74.813194 ],\n",
       "       [46.78692  ],\n",
       "       [64.20712  ],\n",
       "       [31.28025  ],\n",
       "       [83.507195 ],\n",
       "       [63.420452 ],\n",
       "       [11.142409 ],\n",
       "       [68.08517  ],\n",
       "       [72.95657  ],\n",
       "       [97.60319  ],\n",
       "       [94.02493  ],\n",
       "       [41.42869  ],\n",
       "       [26.208673 ],\n",
       "       [ 4.539678 ],\n",
       "       [17.028353 ],\n",
       "       [12.3109455],\n",
       "       [19.025309 ],\n",
       "       [32.096893 ],\n",
       "       [32.67212  ],\n",
       "       [55.340973 ],\n",
       "       [97.3783   ],\n",
       "       [82.7851   ],\n",
       "       [49.757812 ],\n",
       "       [95.16399  ],\n",
       "       [93.97331  ],\n",
       "       [94.453674 ],\n",
       "       [17.870758 ],\n",
       "       [47.26724  ],\n",
       "       [22.352474 ],\n",
       "       [12.117515 ],\n",
       "       [99.97838  ],\n",
       "       [86.66206  ],\n",
       "       [59.100243 ],\n",
       "       [ 1.3934457],\n",
       "       [63.508457 ],\n",
       "       [67.18599  ],\n",
       "       [40.004944 ],\n",
       "       [77.78187  ],\n",
       "       [ 6.3743615],\n",
       "       [38.060444 ],\n",
       "       [59.20889  ],\n",
       "       [43.58877  ],\n",
       "       [58.67892  ],\n",
       "       [84.58928  ],\n",
       "       [66.31045  ],\n",
       "       [94.95295  ],\n",
       "       [14.836266 ],\n",
       "       [87.07825  ],\n",
       "       [45.134323 ],\n",
       "       [84.78553  ],\n",
       "       [33.803257 ],\n",
       "       [58.860645 ],\n",
       "       [ 5.311074 ],\n",
       "       [68.98912  ],\n",
       "       [ 7.0655413],\n",
       "       [81.70937  ],\n",
       "       [45.67921  ],\n",
       "       [55.317074 ],\n",
       "       [ 8.80207  ],\n",
       "       [ 8.986944 ],\n",
       "       [73.87529  ],\n",
       "       [75.07908  ],\n",
       "       [26.310959 ],\n",
       "       [53.577816 ],\n",
       "       [18.14418  ],\n",
       "       [83.34254  ],\n",
       "       [71.24343  ],\n",
       "       [51.925327 ],\n",
       "       [49.79086  ],\n",
       "       [19.423653 ],\n",
       "       [82.5526   ],\n",
       "       [62.306087 ],\n",
       "       [44.859486 ],\n",
       "       [24.963758 ],\n",
       "       [47.741016 ],\n",
       "       [85.60306  ],\n",
       "       [16.986599 ],\n",
       "       [69.74888  ],\n",
       "       [44.786633 ],\n",
       "       [72.76518  ],\n",
       "       [80.66439  ],\n",
       "       [23.735918 ],\n",
       "       [87.25681  ],\n",
       "       [43.57428  ],\n",
       "       [25.36464  ],\n",
       "       [ 8.090886 ],\n",
       "       [86.56798  ],\n",
       "       [70.43952  ],\n",
       "       [ 3.3415556],\n",
       "       [29.742062 ],\n",
       "       [36.237785 ],\n",
       "       [78.82117  ],\n",
       "       [90.51795  ],\n",
       "       [17.183084 ],\n",
       "       [76.94334  ],\n",
       "       [37.07676  ],\n",
       "       [57.441902 ],\n",
       "       [53.211163 ],\n",
       "       [59.23144  ],\n",
       "       [91.630554 ],\n",
       "       [ 7.621874 ],\n",
       "       [ 1.8934138],\n",
       "       [37.05305  ],\n",
       "       [84.36232  ],\n",
       "       [23.500286 ],\n",
       "       [42.852295 ],\n",
       "       [78.61415  ],\n",
       "       [44.09882  ],\n",
       "       [90.938354 ],\n",
       "       [79.52292  ],\n",
       "       [12.154737 ],\n",
       "       [68.4149   ],\n",
       "       [96.802345 ],\n",
       "       [94.12824  ],\n",
       "       [30.894335 ],\n",
       "       [61.45353  ],\n",
       "       [26.588335 ],\n",
       "       [52.74653  ],\n",
       "       [19.725151 ],\n",
       "       [42.09287  ],\n",
       "       [ 4.849048 ],\n",
       "       [89.15388  ],\n",
       "       [55.415592 ],\n",
       "       [81.88043  ],\n",
       "       [52.362312 ],\n",
       "       [94.08648  ],\n",
       "       [86.00499  ],\n",
       "       [87.60849  ],\n",
       "       [55.5927   ],\n",
       "       [ 2.7027993],\n",
       "       [32.277767 ],\n",
       "       [96.30431  ],\n",
       "       [86.126335 ],\n",
       "       [49.470783 ],\n",
       "       [ 3.968227 ],\n",
       "       [34.236168 ],\n",
       "       [ 5.550956 ],\n",
       "       [26.500412 ],\n",
       "       [88.74038  ],\n",
       "       [74.65208  ],\n",
       "       [19.681225 ],\n",
       "       [17.843626 ],\n",
       "       [51.355885 ],\n",
       "       [78.92186  ],\n",
       "       [57.576298 ],\n",
       "       [68.70473  ],\n",
       "       [43.993923 ],\n",
       "       [32.866684 ],\n",
       "       [25.411692 ],\n",
       "       [17.25144  ]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(disc)\n",
    "y = discriminator.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.140653 ],\n",
       "       [9.301202 ],\n",
       "       [9.370041 ],\n",
       "       [9.439928 ],\n",
       "       [9.146412 ],\n",
       "       [9.3129835],\n",
       "       [9.238287 ],\n",
       "       [9.223189 ],\n",
       "       [9.318062 ],\n",
       "       [9.360418 ],\n",
       "       [9.231604 ],\n",
       "       [9.218878 ],\n",
       "       [9.235582 ],\n",
       "       [9.416879 ],\n",
       "       [9.43762  ],\n",
       "       [9.146247 ],\n",
       "       [9.416647 ],\n",
       "       [9.185789 ],\n",
       "       [9.223118 ],\n",
       "       [9.420671 ],\n",
       "       [9.188429 ],\n",
       "       [9.413841 ],\n",
       "       [9.251806 ],\n",
       "       [9.311153 ],\n",
       "       [9.284761 ],\n",
       "       [9.252274 ],\n",
       "       [9.136383 ],\n",
       "       [9.246962 ],\n",
       "       [9.290276 ],\n",
       "       [9.311865 ],\n",
       "       [9.269506 ],\n",
       "       [9.140953 ],\n",
       "       [9.225946 ],\n",
       "       [9.255642 ],\n",
       "       [9.317392 ],\n",
       "       [9.281753 ],\n",
       "       [9.282188 ],\n",
       "       [9.156066 ],\n",
       "       [9.42764  ],\n",
       "       [9.312027 ],\n",
       "       [9.3513775],\n",
       "       [9.196589 ],\n",
       "       [9.198053 ],\n",
       "       [9.257616 ],\n",
       "       [9.220158 ],\n",
       "       [9.271226 ],\n",
       "       [9.157412 ],\n",
       "       [9.207491 ],\n",
       "       [9.317711 ],\n",
       "       [9.4004135],\n",
       "       [9.252093 ],\n",
       "       [9.433567 ],\n",
       "       [9.3069   ],\n",
       "       [9.18609  ],\n",
       "       [9.205175 ],\n",
       "       [9.276979 ],\n",
       "       [9.313896 ],\n",
       "       [9.242519 ],\n",
       "       [9.427517 ],\n",
       "       [9.133975 ],\n",
       "       [9.299782 ],\n",
       "       [9.191229 ],\n",
       "       [9.140856 ],\n",
       "       [9.426214 ],\n",
       "       [9.215116 ],\n",
       "       [9.31377  ],\n",
       "       [9.17419  ],\n",
       "       [9.229018 ],\n",
       "       [9.282211 ],\n",
       "       [9.424309 ],\n",
       "       [9.269234 ],\n",
       "       [9.266804 ],\n",
       "       [9.11521  ],\n",
       "       [9.237982 ],\n",
       "       [9.377289 ],\n",
       "       [9.25591  ],\n",
       "       [9.194916 ],\n",
       "       [9.112885 ],\n",
       "       [9.35057  ],\n",
       "       [9.197388 ],\n",
       "       [9.288124 ],\n",
       "       [9.184925 ],\n",
       "       [9.129083 ],\n",
       "       [9.41694  ],\n",
       "       [9.387725 ],\n",
       "       [9.388811 ],\n",
       "       [9.269094 ],\n",
       "       [9.197569 ],\n",
       "       [9.389738 ],\n",
       "       [9.438707 ],\n",
       "       [9.2236805],\n",
       "       [9.382057 ],\n",
       "       [9.218855 ],\n",
       "       [9.127333 ],\n",
       "       [9.273324 ],\n",
       "       [9.334877 ],\n",
       "       [9.168713 ],\n",
       "       [9.259454 ],\n",
       "       [9.393166 ],\n",
       "       [9.350366 ],\n",
       "       [9.300471 ],\n",
       "       [9.183832 ],\n",
       "       [9.367458 ],\n",
       "       [9.394621 ],\n",
       "       [9.2425785],\n",
       "       [9.28178  ],\n",
       "       [9.281237 ],\n",
       "       [9.226664 ],\n",
       "       [9.268719 ],\n",
       "       [9.218695 ],\n",
       "       [9.373103 ],\n",
       "       [9.377892 ],\n",
       "       [9.317409 ],\n",
       "       [9.3531   ],\n",
       "       [9.40305  ],\n",
       "       [9.165446 ],\n",
       "       [9.376438 ],\n",
       "       [9.406147 ],\n",
       "       [9.391729 ],\n",
       "       [9.398828 ],\n",
       "       [9.241501 ],\n",
       "       [9.419334 ],\n",
       "       [9.261445 ],\n",
       "       [9.185654 ],\n",
       "       [9.165066 ],\n",
       "       [9.211365 ],\n",
       "       [9.340691 ],\n",
       "       [9.422163 ],\n",
       "       [9.232941 ],\n",
       "       [9.21612  ],\n",
       "       [9.33847  ],\n",
       "       [9.249944 ],\n",
       "       [9.428179 ],\n",
       "       [9.201576 ],\n",
       "       [9.361837 ],\n",
       "       [9.326612 ],\n",
       "       [9.301123 ],\n",
       "       [9.31443  ],\n",
       "       [9.243526 ],\n",
       "       [9.129341 ],\n",
       "       [9.331045 ],\n",
       "       [9.267227 ],\n",
       "       [9.238422 ],\n",
       "       [9.414421 ],\n",
       "       [9.115526 ],\n",
       "       [9.427655 ],\n",
       "       [9.379269 ],\n",
       "       [9.239357 ],\n",
       "       [9.211167 ],\n",
       "       [9.184913 ],\n",
       "       [9.35164  ],\n",
       "       [9.255066 ],\n",
       "       [9.311625 ],\n",
       "       [9.209328 ],\n",
       "       [9.38362  ],\n",
       "       [9.308657 ],\n",
       "       [9.144758 ],\n",
       "       [9.326256 ],\n",
       "       [9.344635 ],\n",
       "       [9.432241 ],\n",
       "       [9.42016  ],\n",
       "       [9.239262 ],\n",
       "       [9.194314 ],\n",
       "       [9.122583 ],\n",
       "       [9.164525 ],\n",
       "       [9.148683 ],\n",
       "       [9.171232 ],\n",
       "       [9.211737 ],\n",
       "       [9.213433 ],\n",
       "       [9.280296 ],\n",
       "       [9.431482 ],\n",
       "       [9.38103  ],\n",
       "       [9.263828 ],\n",
       "       [9.4240055],\n",
       "       [9.419986 ],\n",
       "       [9.421608 ],\n",
       "       [9.167355 ],\n",
       "       [9.256482 ],\n",
       "       [9.182265 ],\n",
       "       [9.148032 ],\n",
       "       [9.44026  ],\n",
       "       [9.394941 ],\n",
       "       [9.2923565],\n",
       "       [9.112018 ],\n",
       "       [9.308989 ],\n",
       "       [9.322864 ],\n",
       "       [9.235062 ],\n",
       "       [9.362841 ],\n",
       "       [9.128745 ],\n",
       "       [9.229326 ],\n",
       "       [9.292768 ],\n",
       "       [9.245632 ],\n",
       "       [9.290768 ],\n",
       "       [9.387504 ],\n",
       "       [9.31956  ],\n",
       "       [9.423293 ],\n",
       "       [9.157164 ],\n",
       "       [9.396434 ],\n",
       "       [9.250191 ],\n",
       "       [9.388207 ],\n",
       "       [9.216769 ],\n",
       "       [9.291453 ],\n",
       "       [9.125174 ],\n",
       "       [9.329667 ],\n",
       "       [9.131066 ],\n",
       "       [9.377171 ],\n",
       "       [9.251799 ],\n",
       "       [9.280226 ],\n",
       "       [9.136898 ],\n",
       "       [9.137519 ],\n",
       "       [9.348102 ],\n",
       "       [9.352643 ],\n",
       "       [9.1946335],\n",
       "       [9.275095 ],\n",
       "       [9.168273 ],\n",
       "       [9.38303  ],\n",
       "       [9.338171 ],\n",
       "       [9.270222 ],\n",
       "       [9.263926 ],\n",
       "       [9.172569 ],\n",
       "       [9.380196 ],\n",
       "       [9.304453 ],\n",
       "       [9.24938  ],\n",
       "       [9.190424 ],\n",
       "       [9.25788  ],\n",
       "       [9.391141 ],\n",
       "       [9.164385 ],\n",
       "       [9.332533 ],\n",
       "       [9.249166 ],\n",
       "       [9.343913 ],\n",
       "       [9.373421 ],\n",
       "       [9.186588 ],\n",
       "       [9.397058 ],\n",
       "       [9.24559  ],\n",
       "       [9.191677 ],\n",
       "       [9.13451  ],\n",
       "       [9.394603 ],\n",
       "       [9.335138 ],\n",
       "       [9.11856  ],\n",
       "       [9.204791 ],\n",
       "       [9.22395  ],\n",
       "       [9.366761 ],\n",
       "       [9.4082775],\n",
       "       [9.165045 ],\n",
       "       [9.359676 ],\n",
       "       [9.226425 ],\n",
       "       [9.286492 ],\n",
       "       [9.2740135],\n",
       "       [9.292852 ],\n",
       "       [9.412077 ],\n",
       "       [9.132935 ],\n",
       "       [9.113696 ],\n",
       "       [9.226355 ],\n",
       "       [9.386689 ],\n",
       "       [9.185852 ],\n",
       "       [9.24346  ],\n",
       "       [9.36598  ],\n",
       "       [9.247137 ],\n",
       "       [9.409723 ],\n",
       "       [9.369325 ],\n",
       "       [9.148157 ],\n",
       "       [9.3275   ],\n",
       "       [9.429538 ],\n",
       "       [9.420509 ],\n",
       "       [9.20819  ],\n",
       "       [9.301236 ],\n",
       "       [9.195489 ],\n",
       "       [9.272644 ],\n",
       "       [9.173582 ],\n",
       "       [9.24122  ],\n",
       "       [9.123623 ],\n",
       "       [9.4035845],\n",
       "       [9.280516 ],\n",
       "       [9.377784 ],\n",
       "       [9.27151  ],\n",
       "       [9.420368 ],\n",
       "       [9.392583 ],\n",
       "       [9.398268 ],\n",
       "       [9.281038 ],\n",
       "       [9.116415 ],\n",
       "       [9.212271 ],\n",
       "       [9.4278555],\n",
       "       [9.393019 ],\n",
       "       [9.262981 ],\n",
       "       [9.120665 ],\n",
       "       [9.218046 ],\n",
       "       [9.12598  ],\n",
       "       [9.195226 ],\n",
       "       [9.402163 ],\n",
       "       [9.351032 ],\n",
       "       [9.173434 ],\n",
       "       [9.167263 ],\n",
       "       [9.268541 ],\n",
       "       [9.367141 ],\n",
       "       [9.286924 ],\n",
       "       [9.328594 ],\n",
       "       [9.246828 ],\n",
       "       [9.214007 ],\n",
       "       [9.191824 ],\n",
       "       [9.165275 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = discriminator.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.save('./my_new_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(disc)\n",
    "new_discriminator = disc.Discriminator(quantile=quantile, loss_strategy=strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-26-af832292da6f>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_discriminator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> b dadrah/selection/discriminator:102\n",
      "Breakpoint 2 at /eos/home-k/kiwoznia/dev/data_driven_anomaly_hunting/dadrah/selection/discriminator.py:102\n",
      "ipdb> new_discriminator.load('./my_new_model.h5')\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-16-b579923c21eb>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0;32mimport\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0mnew_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./my_new_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "import ipdb; ipdb.set_trace()\n",
    "new_discriminator.load('./my_new_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "None\n",
      "> \u001b[0;32m<ipython-input-17-fb655b3cf6a1>\u001b[0m(1)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m----> 1 \u001b[0;31m\u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      2 \u001b[0;31m\u001b[0mnew_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "ipdb.set_trace()\n",
    "new_discriminator.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Discriminator' object has no attribute 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-a119eabab929>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloaded_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_discriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Discriminator' object has no attribute 'model'"
     ]
    }
   ],
   "source": [
    "loaded_weights = new_discriminator.model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52151966,  0.34217453,  0.01405535,  0.59535474,  0.69920963,\n",
       "        -0.1658983 , -0.44183353, -0.02300867, -0.06862972, -0.00430826,\n",
       "        -0.24678671,  0.7089294 ,  0.0510585 ,  0.01305069, -0.27448598,\n",
       "        -0.06475051, -0.4706956 , -0.4577948 ,  0.19247283, -0.01148568]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.52151966,  0.34217453,  0.01405535,  0.59535474,  0.69920963,\n",
       "        -0.1658983 , -0.44183353, -0.02300867, -0.06862972, -0.00430826,\n",
       "        -0.24678671,  0.7089294 ,  0.0510585 ,  0.01305069, -0.27448598,\n",
       "        -0.06475051, -0.4706956 , -0.4577948 ,  0.19247283, -0.01148568]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(weights)):\n",
    "    assert np.allclose(weights[0], loaded_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_loaded = loaded_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(y, y_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_n = 2000\n",
    "xx_min = 1200.\n",
    "xx_range = 1000.\n",
    "xx = (np.random.exponential(scale=0.5, size=xx_n)*xx_range)+xx_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([139., 118., 110., 106.,  96.,  82.,  81.,  80.,  72.,  67.,  67.,\n",
       "         71.,  54.,  51.,  48.,  49.,  48.,  44.,  35.,  40.,  36.,  24.,\n",
       "         27.,  22.,  29.,  33.,  21.,  31.,  22.,  28.,  12.,  17.,  19.,\n",
       "         15.,  17.,  14.,  12.,   5.,  14.,   0.,   7.,   9.,   7.,   3.,\n",
       "         10.,   2.,   7.,   2.,   5.,   3.,   6.,   7.,   2.,   6.,   4.,\n",
       "          1.,   2.,   5.,   3.,   3.,   0.,   3.,   1.,   4.,   3.,   1.,\n",
       "          2.,   1.,   4.,   3.,   1.,   2.,   5.,   0.,   0.,   2.,   4.,\n",
       "          3.,   1.,   0.,   1.,   0.,   0.,   1.,   0.,   2.,   0.,   0.,\n",
       "          0.,   2.,   0.,   0.,   1.,   0.,   1.,   0.,   0.,   0.,   1.,\n",
       "          1.]),\n",
       " array([1201.42854264, 1233.88833714, 1266.34813165, 1298.80792615,\n",
       "        1331.26772066, 1363.72751517, 1396.18730967, 1428.64710418,\n",
       "        1461.10689869, 1493.56669319, 1526.0264877 , 1558.4862822 ,\n",
       "        1590.94607671, 1623.40587122, 1655.86566572, 1688.32546023,\n",
       "        1720.78525473, 1753.24504924, 1785.70484375, 1818.16463825,\n",
       "        1850.62443276, 1883.08422726, 1915.54402177, 1948.00381628,\n",
       "        1980.46361078, 2012.92340529, 2045.3831998 , 2077.8429943 ,\n",
       "        2110.30278881, 2142.76258331, 2175.22237782, 2207.68217233,\n",
       "        2240.14196683, 2272.60176134, 2305.06155584, 2337.52135035,\n",
       "        2369.98114486, 2402.44093936, 2434.90073387, 2467.36052837,\n",
       "        2499.82032288, 2532.28011739, 2564.73991189, 2597.1997064 ,\n",
       "        2629.65950091, 2662.11929541, 2694.57908992, 2727.03888442,\n",
       "        2759.49867893, 2791.95847344, 2824.41826794, 2856.87806245,\n",
       "        2889.33785695, 2921.79765146, 2954.25744597, 2986.71724047,\n",
       "        3019.17703498, 3051.63682948, 3084.09662399, 3116.5564185 ,\n",
       "        3149.016213  , 3181.47600751, 3213.93580202, 3246.39559652,\n",
       "        3278.85539103, 3311.31518553, 3343.77498004, 3376.23477455,\n",
       "        3408.69456905, 3441.15436356, 3473.61415806, 3506.07395257,\n",
       "        3538.53374708, 3570.99354158, 3603.45333609, 3635.91313059,\n",
       "        3668.3729251 , 3700.83271961, 3733.29251411, 3765.75230862,\n",
       "        3798.21210313, 3830.67189763, 3863.13169214, 3895.59148664,\n",
       "        3928.05128115, 3960.51107566, 3992.97087016, 4025.43066467,\n",
       "        4057.89045917, 4090.35025368, 4122.81004819, 4155.26984269,\n",
       "        4187.7296372 , 4220.1894317 , 4252.64922621, 4285.10902072,\n",
       "        4317.56881522, 4350.02860973, 4382.48840424, 4414.94819874,\n",
       "        4447.40799325]),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(xx,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_layer = tf.keras.layers.Lambda(lambda x: tf.math.log(x-xx_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lambda_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xx_trans = trans_layer(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01293841, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.0064692 , 0.01940762, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00646921, 0.01940762, 0.00646921,\n",
       "        0.01293841, 0.02587679, 0.01293841, 0.03234604, 0.00646921,\n",
       "        0.01293839, 0.00646921, 0.04528438, 0.01940765, 0.01293839,\n",
       "        0.01293843, 0.02587679, 0.02587679, 0.01940765, 0.03234599,\n",
       "        0.03234609, 0.02587679, 0.03881518, 0.01940765, 0.05175358,\n",
       "        0.04528438, 0.06469217, 0.05175358, 0.05175374, 0.08409956,\n",
       "        0.05822277, 0.05822295, 0.06469197, 0.0388153 , 0.07116117,\n",
       "        0.09056876, 0.11644591, 0.10350716, 0.12291513, 0.12291437,\n",
       "        0.14232278, 0.08409982, 0.1423219 , 0.18113808, 0.15526121,\n",
       "        0.18113808, 0.19407532, 0.21995338, 0.17466886, 0.14879108,\n",
       "        0.25876869, 0.25229947, 0.18113696, 0.25876869, 0.30405321,\n",
       "        0.27170544, 0.29758399, 0.31052242, 0.29758399, 0.31052051,\n",
       "        0.34933773, 0.36227616, 0.37521228, 0.37521459, 0.35580694,\n",
       "        0.37521228, 0.36227616, 0.38168381, 0.37521228, 0.35580694,\n",
       "        0.27170712, 0.32346086, 0.34933557, 0.34933773, 0.32346086,\n",
       "        0.24582874, 0.25229947, 0.16819965, 0.1423219 , 0.10350747,\n",
       "        0.09703826, 0.12938354, 0.09703826, 0.06469217, 0.07116139,\n",
       "        0.09703766, 0.06469217, 0.02587687, 0.01940765, 0.01940753]),\n",
       " array([0.35668716, 0.4339764 , 0.51126564, 0.5885549 , 0.66584414,\n",
       "        0.7431334 , 0.82042265, 0.89771193, 0.97500116, 1.0522904 ,\n",
       "        1.1295797 , 1.2068689 , 1.2841582 , 1.3614475 , 1.4387367 ,\n",
       "        1.5160259 , 1.5933151 , 1.6706045 , 1.7478937 , 1.8251829 ,\n",
       "        1.9024721 , 1.9797615 , 2.0570507 , 2.13434   , 2.2116292 ,\n",
       "        2.2889185 , 2.3662076 , 2.443497  , 2.5207863 , 2.5980754 ,\n",
       "        2.6753647 , 2.7526538 , 2.8299432 , 2.9072325 , 2.9845216 ,\n",
       "        3.061811  , 3.1391003 , 3.2163894 , 3.2936788 , 3.3709679 ,\n",
       "        3.4482572 , 3.5255466 , 3.6028357 , 3.680125  , 3.757414  ,\n",
       "        3.8347034 , 3.9119928 , 3.989282  , 4.066571  , 4.1438603 ,\n",
       "        4.22115   , 4.298439  , 4.375728  , 4.4530177 , 4.530307  ,\n",
       "        4.607596  , 4.684885  , 4.7621746 , 4.8394637 , 4.916753  ,\n",
       "        4.9940424 , 5.0713315 , 5.1486206 , 5.22591   , 5.3031993 ,\n",
       "        5.3804884 , 5.457778  , 5.535067  , 5.612356  , 5.6896453 ,\n",
       "        5.766935  , 5.844224  , 5.921513  , 5.9988027 , 6.076092  ,\n",
       "        6.153381  , 6.2306705 , 6.3079596 , 6.3852487 , 6.4625382 ,\n",
       "        6.5398273 , 6.6171165 , 6.6944056 , 6.771695  , 6.8489842 ,\n",
       "        6.9262733 , 7.003563  , 7.080852  , 7.158141  , 7.2354307 ,\n",
       "        7.31272   , 7.390009  , 7.4672985 , 7.5445876 , 7.6218767 ,\n",
       "        7.699166  , 7.7764554 , 7.8537445 , 7.9310336 , 8.008323  ,\n",
       "        8.085612  ], dtype=float32),\n",
       " <BarContainer object of 100 artists>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(xx_trans.numpy(),bins=100,density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
